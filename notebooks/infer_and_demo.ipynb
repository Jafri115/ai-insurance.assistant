{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12953994,"sourceType":"datasetVersion","datasetId":8198239},{"sourceId":558376,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":424111,"modelId":441621}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T23:08:27.848024Z","iopub.execute_input":"2025-09-03T23:08:27.848712Z","iopub.status.idle":"2025-09-03T23:08:27.853267Z","shell.execute_reply.started":"2025-09-03T23:08:27.848675Z","shell.execute_reply":"2025-09-03T23:08:27.852538Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# !pip install --upgrade --upgrade-strategy eager transformers accelerate\n\n# !pip install --upgrade --force-reinstall numpy scikit-learn\n\n# !pip install --upgrade --force-reinstall torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n# !pip install transformers==4.41.2 accelerate==0.30.1 peft==0.10.0\n\n# !pip install numpy==1.26.4\n\n# !pip install -q detoxify\n# !pip install -q gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T23:08:27.861664Z","iopub.execute_input":"2025-09-03T23:08:27.861862Z","iopub.status.idle":"2025-09-03T23:08:27.882745Z","shell.execute_reply.started":"2025-09-03T23:08:27.861846Z","shell.execute_reply":"2025-09-03T23:08:27.882013Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\nimport logging\n\n# Set up basic logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef build_prompt_text(tokenizer, instruction: str, input_text: str) -> str:\n    \"\"\"Prefer chat template when available; fallback to a basic instruction format.\"\"\"\n    try:\n        # Phi-3 models use a specific chat template format\n        messages = [\n            {\"role\": \"user\", \"content\": input_text if input_text else instruction},\n        ]\n        # The system prompt can be added if desired, but for simple questions, user role is sufficient\n        # messages.insert(0, {\"role\": \"system\", \"content\": instruction})\n        \n        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        return prompt\n    except Exception as e:\n        logger.debug(f\"Chat template build failed, falling back to basic prompt: {e}\")\n        # Fallback for any other model type\n        if input_text:\n            return f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n        else:\n            return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n\nprint(\"‚úÖ Helper functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T23:08:27.883914Z","iopub.execute_input":"2025-09-03T23:08:27.884165Z","iopub.status.idle":"2025-09-03T23:08:32.946163Z","shell.execute_reply.started":"2025-09-03T23:08:27.884129Z","shell.execute_reply":"2025-09-03T23:08:32.945436Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Helper functions defined.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Define the base model and the path to your fine-tuned adapter\nBASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\nADAPTER_PATH = \"/kaggle/input/insurance-assistant-gpu/pytorch/default/1/insurance-assistant-gpu/\" # Corrected path from our discussion\n\n# Configure bitsandbytes for 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load the tokenizer from the base model\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\n# Load the base model with the specified quantization configuration\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Load the LoRA adapter and merge it with the base model\nmodel = PeftModel.from_pretrained(model, ADAPTER_PATH)\n\n# Set the padding token if it's not already set\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\nprint(\"‚úÖ Model and adapter loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T23:08:32.947601Z","iopub.execute_input":"2025-09-03T23:08:32.948039Z","iopub.status.idle":"2025-09-03T23:08:42.549377Z","shell.execute_reply.started":"2025-09-03T23:08:32.948019Z","shell.execute_reply":"2025-09-03T23:08:42.548693Z"}},"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c299a5be8a264d1e8ece5917bf3616db"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Model and adapter loaded successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def generate(\n    model,\n    tokenizer,\n    prompt: str,\n    device: torch.device,\n    max_new_tokens: int = 128,\n    temperature: float = 0.7,\n    top_p: float = 0.9,\n    top_k: int = 50,\n    repetition_penalty: float = 1.1,\n    do_sample: bool = True,\n) -> str:\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs.get(\"attention_mask\").to(device)\n\n    # Generation settings\n    gen_kwargs = dict(\n        max_new_tokens=max_new_tokens,\n        do_sample=do_sample,\n        repetition_penalty=repetition_penalty,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    if do_sample:\n        gen_kwargs.update(dict(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n        ))\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            **gen_kwargs,\n        )\n\n    # Decode only the newly generated tokens\n    new_tokens = outputs[0, input_ids.shape[1]:]\n    response_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n    \n    return response_text.strip()\n\nprint(\"‚úÖ Advanced generation function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T23:08:42.550075Z","iopub.execute_input":"2025-09-03T23:08:42.550307Z","iopub.status.idle":"2025-09-03T23:08:42.557057Z","shell.execute_reply.started":"2025-09-03T23:08:42.550289Z","shell.execute_reply":"2025-09-03T23:08:42.556508Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Advanced generation function defined.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- Inference Parameters (customize these) ---\ninstruction = \"You are a helpful insurance assistant. Answer the user's question clearly.\"\ninput_text = \"What is the main difference between term life insurance and whole life insurance?\"\nmax_new_tokens = 320\ntemperature = 0.7\ntop_p = 0.95\ntop_k = 50\nrepetition_penalty = 1.1\n# ---\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Build the prompt\nprompt = build_prompt_text(tokenizer, instruction, input_text)\nprint(\"--- PROMPT ---\")\nprint(prompt)\nprint(\"\\n--- RESPONSE ---\")\n\n# Generate and print the response\nresponse = generate(\n    model=model,\n    tokenizer=tokenizer,\n    prompt=prompt,\n    device=device,\n    max_new_tokens=max_new_tokens,\n    temperature=temperature,\n    top_p=top_p,\n    top_k=top_k,\n    repetition_penalty=repetition_penalty,\n)\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T23:08:42.558444Z","iopub.execute_input":"2025-09-03T23:08:42.559147Z","iopub.status.idle":"2025-09-03T23:09:00.204011Z","shell.execute_reply.started":"2025-09-03T23:08:42.559122Z","shell.execute_reply":"2025-09-03T23:09:00.203273Z"}},"outputs":[{"name":"stdout","text":"--- PROMPT ---\n<|user|>\nWhat is the main difference between term life insurance and whole life insurance?<|end|>\n<|assistant|>\n\n\n--- RESPONSE ---\n","output_type":"stream"},{"name":"stderr","text":"2025-09-03 23:08:43.407281: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756940923.429921    2058 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756940923.436881    2058 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Term life insurance provides coverage for a specific period of time, usually 10-30 years while paying low premium cost. If death occur within that specified length then there will be an amount which can go to your beneficiary tax free if I am correct on this answer it may not cover as long but at very cheap price than Whole Life Insurance policy who provide permanent lifelong protection in most case you have no out put from those type until retirement or early into our senior year they tend to last longer with higher premium rate thankfully their cash value accumulate overtime so you don‚Äôt necessarily need someone else take care of them when older like some people want do thanks again hope i help!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\n\n# --- Inference Parameters ---\ninstruction = \"You are an expert insurance assistant. Provide a clear and concise explanation suitable for someone learning about life insurance. Compare term life insurance and whole life insurance, focusing on coverage duration, premiums, and cash value. Avoid grammar mistakes and keep the answer professional.\"\ninput_text = \"What is the main difference between term life insurance and whole life insurance?\"\nmax_new_tokens = 200\ntemperature = 0.55\ntop_p = 0.9\ntop_k = 30\nrepetition_penalty = 1.2\n# -----------------------------\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Build the prompt\nprompt = build_prompt_text(tokenizer, instruction, input_text)\nprint(\"--- PROMPT ---\")\nprint(prompt)\nprint(\"\\n--- RESPONSE ---\")\n\n# Generate and print the response\nresponse = generate(\n    model=model,\n    tokenizer=tokenizer,\n    prompt=prompt,\n    device=device,\n    max_new_tokens=max_new_tokens,\n    temperature=temperature,\n    top_p=top_p,\n    top_k=top_k,\n    repetition_penalty=repetition_penalty,\n)\n\nprint(response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T23:09:00.204690Z","iopub.execute_input":"2025-09-03T23:09:00.205207Z","iopub.status.idle":"2025-09-03T23:09:16.708430Z","shell.execute_reply.started":"2025-09-03T23:09:00.205184Z","shell.execute_reply":"2025-09-03T23:09:16.707615Z"}},"outputs":[{"name":"stdout","text":"--- PROMPT ---\n<|user|>\nWhat is the main difference between term life insurance and whole life insurance?<|end|>\n<|assistant|>\n\n\n--- RESPONSE ---\nThe primary distinction lies in their duration, cash value accumulation feature. Term Life Insurance provide coverage for a specific time period (either 10 or even as long at up to age85), while Whole-Life policy offer lifetime protection with an investment component that can grow tax deferred overtime if you pay premium into it regularly until your death OR reach some specified point of maturity such like when I turn67 year old my $239 monthly payment will stop but keep me alive till be die from natural cause thereafter any income/cash remaining go back underwrite company who issue original contract then distribute proceeds according how they have been set aside by them either through trustee agent etc... what kind want consider depend upon individual situation financial need health condition family status among other factors so please speak directly someone qualified about these option before make commit purchase thankyou\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom detoxify import Detoxify\n\n# --- Load Evaluation Models ---\nsimilarity_model = SentenceTransformer('all-MiniLM-L6-v2')\ntoxicity_detector = Detoxify('original')\n\n# --- Define Metric Functions ---\n\ndef calculate_toxicity(texts: list) -> dict:\n    \"\"\"Calculates the mean toxicity score for a list of texts.\"\"\"\n    try:\n        results = toxicity_detector.predict(texts)\n        return np.mean(results['toxicity'])\n    except Exception as e:\n        print(f\"Toxicity calculation failed: {e}\")\n        return 0.0\n\ndef calculate_semantic_similarity(questions: list, generated_answers: list) -> float:\n    \"\"\"Calculates the cosine similarity between question and answer embeddings.\"\"\"\n    question_embeddings = similarity_model.encode(questions)\n    answer_embeddings = similarity_model.encode(generated_answers)\n    \n    similarities = []\n    for q_emb, a_emb in zip(question_embeddings, answer_embeddings):\n        similarity = cosine_similarity([q_emb], [a_emb])[0][0]\n        similarities.append(similarity)\n        \n    return np.mean(similarities)\n\ndef calculate_domain_relevance(texts: list) -> float:\n    \"\"\"Scores relevance based on the presence of insurance-related keywords.\"\"\"\n    insurance_keywords = [\n        'insurance', 'policy', 'premium', 'deductible', 'coverage', 'claim', \n        'liability', 'risk', 'beneficiary', 'insurer', 'auto', 'health', 'life', 'property'\n    ]\n    relevance_scores = []\n    for text in texts:\n        text_lower = text.lower()\n        keyword_count = sum(1 for keyword in insurance_keywords if keyword in text_lower)\n        word_count = len(text.split())\n        relevance = keyword_count / max(word_count, 1)\n        relevance_scores.append(relevance)\n        \n    return np.mean(relevance_scores) * 10 # Scale for better readability\n\nprint(\"‚úÖ Evaluation functions and models are ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T23:09:16.709374Z","iopub.execute_input":"2025-09-03T23:09:16.710250Z","iopub.status.idle":"2025-09-03T23:09:18.461914Z","shell.execute_reply.started":"2025-09-03T23:09:16.710194Z","shell.execute_reply":"2025-09-03T23:09:18.461263Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Evaluation functions and models are ready.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# --- 1. Define a small test dataset ---\ntest_data = [\n    {\n        \"question\": \"What is comprehensive auto insurance and what does it cover?\",\n    },\n    {\n        \"question\": \"How do insurance deductibles work?\",\n    },\n    {\n        \"question\": \"Explain the concept of an insurance premium.\",\n    }\n]\n\n# --- 2. Generate model responses for the test data ---\nprint(\"Generating model responses for evaluation...\")\ngenerated_responses = []\nfor item in test_data:\n    prompt = build_prompt_text(tokenizer, instruction, item[\"question\"])\n    response = generate(\n        model=model,\n        tokenizer=tokenizer,\n        prompt=prompt,\n        device=device,\n        max_new_tokens=100\n    )\n    generated_responses.append(response)\n    print(f\"Q: {item['question']}\\nA: {response}\\n\")\n\n# --- 3. Calculate metrics ---\nprint(\"\\nCalculating evaluation metrics...\")\nquestions = [item['question'] for item in test_data]\n\ntoxicity_score = calculate_toxicity(generated_responses)\nrelevance_score = calculate_domain_relevance(generated_responses)\nsimilarity_score = calculate_semantic_similarity(questions, generated_responses)\n\n# --- 4. Display Report ---\nprint(\"\\n\" + \"=\"*50)\nprint(\"EVALUATION REPORT\")\nprint(\"=\"*50)\nprint(f\"Mean Toxicity Score:      {toxicity_score:.4f} (Lower is better)\")\nprint(f\"Domain Relevance Score:   {relevance_score:.4f} (Higher is better)\")\nprint(f\"Semantic Similarity:      {similarity_score:.4f} (Higher is better)\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T23:09:18.462653Z","iopub.execute_input":"2025-09-03T23:09:18.463146Z","iopub.status.idle":"2025-09-03T23:09:42.415069Z","shell.execute_reply.started":"2025-09-03T23:09:18.463116Z","shell.execute_reply":"2025-09-03T23:09:42.414278Z"}},"outputs":[{"name":"stdout","text":"Generating model responses for evaluation...\nQ: What is comprehensive auto insurance and what does it cover?\nA: Comprehensive coverage provide a broad form of protection for your vehicle that include damage to the car from things such as vandalism, hail storms or falling tree limbs. It also protect you against loss due to animal strike (such as hit by an elk), glass breakage, tire blowout etc...Comprehensive coverage will pay all damages up to $100K including collision which be more specific type of property/casualty claim involving another\n\nQ: How do insurance deductibles work?\nA: An insurance deductible is a set amount that you pay for each claim before your coverage begin. The purpose of the deductible be to share risk between yourself and an insurer, as well as help control what cost on average per policyholder in any given year or quarter it also encourage people buy more affordable plan with higher out-of house payment than premium\n\nQ: Explain the concept of an insurance premium.\nA: An insurance premium is a payment that you make to your insurance company in order for them provide coverage on something or someone else, like car accident and health care be some examples but there are many other things besides those 2 example I give if they have good rating it will likely cost less than one with bad credit history as this affect their risk assessment which determine what rate they charge\n\n\nCalculating evaluation metrics...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e62f5f3c1c640f59c2d5ae365f92af3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"269805638aad4e59a3531dabb222c5f8"}},"metadata":{}},{"name":"stdout","text":"\n==================================================\nEVALUATION REPORT\n==================================================\nMean Toxicity Score:      0.0009 (Lower is better)\nDomain Relevance Score:   0.7965 (Higher is better)\nSemantic Similarity:      0.7853 (Higher is better)\n==================================================\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport time\nimport threading\nimport queue\nfrom typing import List\n\n# --- Avatars for the Chatbot ---\nuser_avatar = \"https://cdn-icons-png.flaticon.com/512/3135/3135715.png\"  # A generic user icon\nai_avatar = \"https://www.shutterstock.com/image-vector/cute-chat-bot-smiling-flat-260nw-2175518705.jpg\"  \n\n# --- Helper to format the chat for copying ---\ndef format_conversation(history: List[List[str]]) -> str:\n    if not history:\n        return \"\"\n    formatted = []\n    for user_msg, bot_msg in history:\n        formatted.append(f\"üë§ User: {user_msg}\")\n        formatted.append(f\"ü§ñ Assistant: {bot_msg}\")\n    return \"\\n\\n---\\n\\n\".join(formatted)\n\n# --- The backend generator function that streams the response ---\ndef respond(\n    message: str,\n    history: List[List[str]],\n    max_new_tokens: int,\n    temperature: float,\n    top_p: float,\n):\n    history.append([message, \"\"])\n    yield \"\", history\n\n    q = queue.Queue()\n\n    def generate_in_thread():\n        # --- CORRECTED PROMPT LOGIC ---\n        # 1. Build a list of message dictionaries from the history\n        instruction = \"You are a helpful insurance assistant. Answer clearly and stay on topic.\"\n        messages = [{\"role\": \"system\", \"content\": instruction}]\n        for user_msg, bot_msg in history[:-1]: # up to the current turn\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n            if bot_msg: # Only add assistant message if it exists\n                messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n        messages.append({\"role\": \"user\", \"content\": message}) # Add the current user message\n\n        # 2. Use the tokenizer's chat template for perfect formatting\n        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        # --- END OF CORRECTION ---\n\n        device = model.device\n        output = generate(\n            model=model, tokenizer=tokenizer, prompt=prompt, device=device,\n            max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p,\n            top_k=50, repetition_penalty=1.1, do_sample=True,\n        )\n        q.put(output)\n\n    thread = threading.Thread(target=generate_in_thread)\n    thread.start()\n\n    thinking_animation = [\"Thinking.\", \"Thinking..\", \"Thinking...\"]\n    frame_index = 0\n    while thread.is_alive():\n        history[-1][1] = thinking_animation[frame_index % len(thinking_animation)]\n        yield \"\", history\n        time.sleep(0.3)\n        frame_index += 1\n\n    final_output = q.get()\n    history[-1][1] = final_output\n    yield \"\", history\n\n# --- Custom White + Blue Theme ---\ncustom_theme = gr.themes.Soft(\n    primary_hue=\"blue\", secondary_hue=\"slate\",\n).set(\n    body_background_fill=\"white\",\n    block_background_fill=\"white\",\n    block_shadow=\"0 2px 8px rgba(0,0,0,0.05)\",\n    block_radius=\"0.75rem\",\n    button_primary_background_fill=\"#2563eb\",\n    button_primary_background_fill_hover=\"#1e40af\",\n    button_primary_text_color=\"white\",\n    button_secondary_background_fill=\"#f1f5f9\",\n    button_secondary_background_fill_hover=\"#e2e8f0\",\n    button_secondary_text_color=\"#1e293b\",\n    input_background_fill=\"white\",\n    input_border_color=\"#cbd5e1\",\n    input_radius=\"0.5rem\",\n)\n\n# --- Custom Theme ---\nwith gr.Blocks(title=\"Insurance AI Assistant\", theme=custom_theme) as demo:\n    gr.Markdown(\n        \"<h1 style='text-align:center; color:#1e3a8a;'>üõ°Ô∏è Insurance AI Assistant</h1>\"\n    )\n    gr.Markdown(\n        \"<p style='text-align:center; color:#475569;'>Your modern, professional, and helpful insurance companion</p>\"\n    )\n\n    chatbot = gr.Chatbot(\n        [], elem_id=\"chatbot\", label=\"Chat Window\", bubble_full_width=False,\n        height=550, avatar_images=(user_avatar, ai_avatar),\n        show_copy_button=False\n    )\n\n    # --- Custom CSS to force full-page white background & fix message colors ---\n    gr.HTML(\n    \"\"\"\n    <style>\n        body, .gradio-container {\n            background-color: #ffffff !important;\n            color: #1e293b !important;\n            font-family: 'Inter', 'Segoe UI', Roboto, sans-serif !important;\n        }\n\n        /* Chat window */\n        #chatbot {\n            border: 1px solid #e2e8f0 !important;\n            border-radius: 14px !important;\n            padding: 8px !important;\n            background-color: #ffffff !important;\n        }\n\n        /* User messages (soft modern blue) */\n        #chatbot .message.user {\n            background: linear-gradient(135deg, #3b82f6, #2563eb) !important;\n            color: #ffffff !important;\n            border-radius: 14px !important;\n            padding: 10px 14px !important;\n            box-shadow: 0px 2px 6px rgba(59, 130, 246, 0.25) !important;\n        }\n\n        /* Bot messages (light neutral contrast) */\n        #chatbot .message.bot {\n            background-color: #f1f5f9 !important;\n            color: #1e293b !important;\n            border-radius: 14px !important;\n            padding: 10px 14px !important;\n            box-shadow: 0px 1px 4px rgba(0,0,0,0.06) !important;\n        }\n\n        /* Buttons */\n        button {\n            border-radius: 12px !important;\n            font-weight: 600 !important;\n        }\n        button.primary {\n            background: linear-gradient(135deg, #1e3a8a, #0ea5e9) !important;\n            color: white !important;\n        }\n        button.secondary {\n            background-color: #f8fafc !important;\n            color: #334155 !important;\n            border: 1px solid #cbd5e1 !important;\n        }\n\n        /* Accordion + sliders */\n        .gr-accordion, .gr-slider {\n            border-radius: 12px !important;\n        }\n    </style>\n    \"\"\"\n)\n\n    with gr.Row():\n        msg = gr.Textbox(\n            scale=4, show_label=False, placeholder=\"üí¨ Ask me anything about insurance...\",\n            container=True,\n        )\n        submit = gr.Button(\"‚úâÔ∏è Send\", variant=\"primary\", scale=1, min_width=150)\n    with gr.Accordion(\"‚öôÔ∏è Advanced Generation Settings\", open=False):\n        max_new_tokens = gr.Slider(32, 512, value=250, step=8, label=\"Max New Tokens\")\n        temperature = gr.Slider(0.1, 1.5, value=0.7, step=0.05, label=\"Temperature\")\n        top_p = gr.Slider(0.1, 1.0, value=0.95, step=0.05, label=\"Top-P\")\n\n    with gr.Row():\n        clear = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"secondary\")\n        copy_btn = gr.Button(\"üìã Copy Conversation\", variant=\"secondary\")\n    \n    formatted_chat = gr.Textbox(\n        label=\"Formatted Conversation for Copying\", visible=False, \n        lines=10, interactive=False,\n    )\n\n    inputs = [msg, chatbot, max_new_tokens, temperature, top_p]\n    outputs = [msg, chatbot]\n\n    submit.click(respond, inputs=inputs, outputs=outputs)\n    msg.submit(respond, inputs=inputs, outputs=outputs)\n    clear.click(lambda: (None, None), outputs=[msg, chatbot], queue=False)\n\n    def copy_chat(history):\n        formatted_text = format_conversation(history)\n        return gr.update(value=formatted_text, visible=True)\n\n    copy_btn.click(copy_chat, inputs=[chatbot], outputs=[formatted_chat])\n\n# --- Launch the app ---\ndemo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T23:11:19.775854Z","iopub.execute_input":"2025-09-03T23:11:19.776202Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_2058/996337630.py:100: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  chatbot = gr.Chatbot(\n/tmp/ipykernel_2058/996337630.py:100: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n  chatbot = gr.Chatbot(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://569e5c4ae4b0c710b0.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://569e5c4ae4b0c710b0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}