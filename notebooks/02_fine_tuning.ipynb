{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed422dd3",
   "metadata": {},
   "source": [
    "# Insurance AI Assistant - Model Fine-tuning with LoRA/QLoRA\n",
    "\n",
    "This notebook demonstrates fine-tuning large language models for insurance-specific tasks using LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) techniques.\n",
    "\n",
    "## Objectives\n",
    "- Load and configure base models (LLaMA, Mistral, Gemma)\n",
    "- Implement QLoRA for memory-efficient fine-tuning\n",
    "- Train the model on insurance datasets\n",
    "- Monitor training progress and performance\n",
    "- Save and evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7080047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for fine-tuning (uncomment if needed)\n",
    "# !pip install transformers peft accelerate torch datasets wandb bitsandbytes\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and training libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Optional: Weights & Biases for experiment tracking\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "    print(\"W&B not available - training will proceed without logging\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\" GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\" GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(\" Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5d009",
   "metadata": {},
   "source": [
    "## Model Configuration and Loading\n",
    "\n",
    "Let's configure and load our base model with QLoRA settings for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b269ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"microsoft/DialoGPT-medium\"  # Change to your preferred model\n",
    "OUTPUT_DIR = \"../models/insurance-assistant\"\n",
    "DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # Adjust based on model architecture\n",
    "\n",
    "print(f\" Model: {MODEL_NAME}\")\n",
    "print(f\" Output directory: {OUTPUT_DIR}\")\n",
    "print(f\" Training config: {NUM_EPOCHS} epochs, batch size {BATCH_SIZE}, LR {LEARNING_RATE}\")\n",
    "print(f\" LoRA config: r={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8002b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\" Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\" Added padding token\")\n",
    "\n",
    "print(\" Loading model with QLoRA configuration...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\" Preparing model for k-bit training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "print(\" Adding LoRA adapters...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "print(\" Model setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89185e",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preprocessing\n",
    "\n",
    "Load and preprocess the insurance training data for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc810698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_json_dataset(file_path):\n",
    "    \"\"\"Load dataset from JSON file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "# Load training and validation data\n",
    "train_path = Path(DATA_DIR) / \"train.json\"\n",
    "val_path = Path(DATA_DIR) / \"validation.json\"\n",
    "\n",
    "if train_path.exists():\n",
    "    train_dataset = load_json_dataset(train_path)\n",
    "    print(f\" Loaded {len(train_dataset)} training samples\")\n",
    "else:\n",
    "    print(\" Training data not found! Please run the data exploration notebook first.\")\n",
    "\n",
    "if val_path.exists():\n",
    "    eval_dataset = load_json_dataset(val_path)\n",
    "    print(f\" Loaded {len(eval_dataset)} validation samples\")\n",
    "else:\n",
    "    print(\"Ô∏è Validation data not found - will use training data split\")\n",
    "    split_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = split_dataset['train']\n",
    "    eval_dataset = split_dataset['test']\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess data for training.\"\"\"\n",
    "    texts = []\n",
    "    for i in range(len(examples['instruction'])):\n",
    "        instruction = examples['instruction'][i]\n",
    "        input_text = examples['input'][i]\n",
    "        output_text = examples['output'][i]\n",
    "        \n",
    "        # Create Alpaca-style prompt\n",
    "        if input_text.strip():\n",
    "            prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output_text}\"\n",
    "        else:\n",
    "            prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output_text}\"\n",
    "        \n",
    "        texts.append(prompt)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    # Set labels for causal LM\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Preprocess datasets\n",
    "print(\" Preprocessing datasets...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\" Preprocessed {len(train_dataset)} training samples\")\n",
    "print(f\" Preprocessed {len(eval_dataset)} validation samples\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}